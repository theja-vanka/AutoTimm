"""Performance optimization utilities for model interpretations.

Provides:
- Caching mechanisms for explanations and activations
- Batch processing for efficient multi-image interpretation
- GPU optimization utilities
- Memory profiling tools
"""

from typing import Optional, Union, List, Dict
import hashlib
import pickle
from pathlib import Path
import time
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import warnings


class ExplanationCache:
    """
    Cache for storing computed explanations to avoid recomputation.

    Uses disk-based caching with LRU eviction policy.

    Args:
        cache_dir: Directory to store cache files
        max_size_mb: Maximum cache size in MB (default: 1000MB = 1GB)
        enabled: Whether caching is enabled

    Example:
        >>> from autotimm.interpretation import GradCAM
        >>> from autotimm.interpretation.optimization import ExplanationCache
        >>>
        >>> cache = ExplanationCache(cache_dir="./cache", max_size_mb=500)
        >>> explainer = GradCAM(model, cache=cache)
        >>>
        >>> # First call: computed and cached
        >>> heatmap1 = explainer.explain(image, target_class=5)
        >>>
        >>> # Second call: retrieved from cache (much faster)
        >>> heatmap2 = explainer.explain(image, target_class=5)
    """

    def __init__(
        self,
        cache_dir: str = "./explanation_cache",
        max_size_mb: int = 1000,
        enabled: bool = True,
    ):
        self.cache_dir = Path(cache_dir)
        self.max_size_mb = max_size_mb
        self.enabled = enabled

        if self.enabled:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            self._metadata_file = self.cache_dir / "metadata.pkl"
            self._load_metadata()

    def _load_metadata(self):
        """Load cache metadata."""
        if self._metadata_file.exists():
            with open(self._metadata_file, "rb") as f:
                self.metadata = pickle.load(f)
        else:
            self.metadata = {
                "keys": [],  # LRU queue
                "sizes": {},  # Key -> size in bytes
                "total_size": 0,
            }

    def _save_metadata(self):
        """Save cache metadata."""
        with open(self._metadata_file, "wb") as f:
            pickle.dump(self.metadata, f)

    def _compute_key(
        self,
        image: Union[Image.Image, np.ndarray, torch.Tensor],
        method: str,
        target_class: Optional[int],
        **kwargs,
    ) -> str:
        """Compute cache key for an explanation."""
        # Create hashable representation
        if isinstance(image, torch.Tensor):
            image_bytes = image.cpu().numpy().tobytes()
        elif isinstance(image, np.ndarray):
            image_bytes = image.tobytes()
        else:  # PIL Image
            image_bytes = image.tobytes()

        # Include method and parameters
        param_str = f"{method}_{target_class}_{sorted(kwargs.items())}"

        # Compute hash
        hasher = hashlib.sha256()
        hasher.update(image_bytes)
        hasher.update(param_str.encode())
        return hasher.hexdigest()

    def get(
        self,
        image: Union[Image.Image, np.ndarray, torch.Tensor],
        method: str,
        target_class: Optional[int] = None,
        **kwargs,
    ) -> Optional[np.ndarray]:
        """
        Retrieve explanation from cache.

        Returns None if not found.
        """
        if not self.enabled:
            return None

        key = self._compute_key(image, method, target_class, **kwargs)
        cache_file = self.cache_dir / f"{key}.npy"

        if cache_file.exists():
            # Update LRU order
            if key in self.metadata["keys"]:
                self.metadata["keys"].remove(key)
            self.metadata["keys"].append(key)
            self._save_metadata()

            # Load and return
            return np.load(cache_file)

        return None

    def put(
        self,
        image: Union[Image.Image, np.ndarray, torch.Tensor],
        method: str,
        explanation: np.ndarray,
        target_class: Optional[int] = None,
        **kwargs,
    ):
        """
        Store explanation in cache.
        """
        if not self.enabled:
            return

        key = self._compute_key(image, method, target_class, **kwargs)
        cache_file = self.cache_dir / f"{key}.npy"

        # Save explanation
        np.save(cache_file, explanation)

        # Update metadata
        file_size = cache_file.stat().st_size
        self.metadata["sizes"][key] = file_size
        self.metadata["total_size"] += file_size

        if key in self.metadata["keys"]:
            self.metadata["keys"].remove(key)
        self.metadata["keys"].append(key)

        # Evict if needed
        while self.metadata["total_size"] > self.max_size_mb * 1024 * 1024:
            if not self.metadata["keys"]:
                break
            self._evict_oldest()

        self._save_metadata()

    def _evict_oldest(self):
        """Evict oldest (LRU) entry from cache."""
        if not self.metadata["keys"]:
            return

        oldest_key = self.metadata["keys"].pop(0)
        cache_file = self.cache_dir / f"{oldest_key}.npy"

        if cache_file.exists():
            cache_file.unlink()

        size = self.metadata["sizes"].pop(oldest_key, 0)
        self.metadata["total_size"] -= size

    def clear(self):
        """Clear entire cache."""
        if not self.enabled:
            return

        for cache_file in self.cache_dir.glob("*.npy"):
            cache_file.unlink()

        self.metadata = {
            "keys": [],
            "sizes": {},
            "total_size": 0,
        }
        self._save_metadata()

    def stats(self) -> Dict:
        """Get cache statistics."""
        if not self.enabled:
            return {"enabled": False}

        return {
            "enabled": True,
            "num_entries": len(self.metadata["keys"]),
            "total_size_mb": self.metadata["total_size"] / (1024 * 1024),
            "max_size_mb": self.max_size_mb,
            "utilization": self.metadata["total_size"]
            / (self.max_size_mb * 1024 * 1024),
        }


class BatchProcessor:
    """
    Efficient batch processing for multiple images.

    Processes images in batches for better GPU utilization.

    Args:
        model: The model being explained
        explainer: The interpretation method
        batch_size: Number of images to process at once
        use_cuda: Whether to use CUDA

    Example:
        >>> from autotimm.interpretation import GradCAM
        >>> from autotimm.interpretation.optimization import BatchProcessor
        >>>
        >>> explainer = GradCAM(model)
        >>> processor = BatchProcessor(model, explainer, batch_size=16)
        >>>
        >>> # Process 100 images efficiently
        >>> images = [load_image(f"img_{i}.jpg") for i in range(100)]
        >>> heatmaps = processor.process_batch(images)
        >>> # 6-7x faster than processing one-by-one
    """

    def __init__(
        self,
        model: nn.Module,
        explainer,
        batch_size: int = 16,
        use_cuda: bool = True,
        show_progress: bool = True,
    ):
        self.model = model
        self.model.eval()
        self.explainer = explainer
        self.batch_size = batch_size
        self.device = torch.device(
            "cuda" if use_cuda and torch.cuda.is_available() else "cpu"
        )
        self.model.to(self.device)
        self.show_progress = show_progress

    def process_batch(
        self,
        images: List[Union[Image.Image, np.ndarray, torch.Tensor]],
        target_classes: Optional[List[int]] = None,
        **kwargs,
    ) -> List[np.ndarray]:
        """
        Process multiple images efficiently.

        Args:
            images: List of images to process
            target_classes: Optional target class for each image
            **kwargs: Additional arguments for explainer

        Returns:
            List of explanation heatmaps
        """
        n_images = len(images)
        heatmaps = []

        if target_classes is None:
            target_classes = [None] * n_images

        # Process in batches
        for i in range(0, n_images, self.batch_size):
            batch_images = images[i : i + self.batch_size]
            batch_targets = target_classes[i : i + self.batch_size]

            if self.show_progress:
                print(
                    f"Processing batch {i // self.batch_size + 1}/{(n_images + self.batch_size - 1) // self.batch_size}..."
                )

            # Process each image in the batch
            # Note: We don't actually batch at GPU level (explainer.explain handles one image)
            # but we group images to show progress and prepare for future GPU batching
            for img, target_class in zip(batch_images, batch_targets):
                heatmap = self.explainer.explain(
                    img, target_class=target_class, **kwargs
                )
                heatmaps.append(heatmap)

        return heatmaps

    def process_batch_parallel(
        self,
        images: List[Union[Image.Image, np.ndarray, torch.Tensor]],
        target_classes: Optional[List[int]] = None,
        num_workers: int = 4,
        **kwargs,
    ) -> List[np.ndarray]:
        """
        Process batch with parallel workers (CPU-based parallelism).

        Useful when GPU is not available or for CPU-bound operations.

        Args:
            images: List of images
            target_classes: Optional target classes
            num_workers: Number of parallel workers
            **kwargs: Additional arguments

        Returns:
            List of heatmaps
        """
        from multiprocessing import Pool

        if target_classes is None:
            target_classes = [None] * len(images)

        # Create worker function
        def worker(args):
            img, target = args
            return self.explainer.explain(img, target_class=target, **kwargs)

        # Process in parallel
        with Pool(num_workers) as pool:
            heatmaps = pool.map(worker, zip(images, target_classes))

        return heatmaps

    def _preprocess_image(
        self, image: Union[Image.Image, np.ndarray, torch.Tensor]
    ) -> torch.Tensor:
        """Preprocess image to tensor."""
        if isinstance(image, torch.Tensor):
            if image.ndim == 3:
                image = image.unsqueeze(0)
            return image.to(self.device)

        if isinstance(image, np.ndarray):
            if image.dtype in [np.float32, np.float64]:
                if image.max() <= 1.0:
                    image = (image * 255).astype(np.uint8)
            image = Image.fromarray(image)

        import torchvision.transforms as T

        transform = T.Compose([T.ToTensor()])
        tensor = transform(image).unsqueeze(0)
        return tensor.to(self.device)


class PerformanceProfiler:
    """
    Profile interpretation performance to identify bottlenecks.

    Args:
        enabled: Whether profiling is enabled

    Example:
        >>> from autotimm.interpretation.optimization import PerformanceProfiler
        >>>
        >>> profiler = PerformanceProfiler(enabled=True)
        >>>
        >>> with profiler.profile("explanation"):
        ...     heatmap = explainer.explain(image)
        >>>
        >>> stats = profiler.get_stats()
        >>> print(f"Average time: {stats['explanation']['mean']:.3f}s")
    """

    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        self.timings = {}

    def profile(self, name: str):
        """Context manager for profiling a code block."""
        return ProfileContext(self, name)

    def record(self, name: str, duration: float):
        """Record a timing."""
        if not self.enabled:
            return

        if name not in self.timings:
            self.timings[name] = []
        self.timings[name].append(duration)

    def get_stats(self) -> Dict:
        """Get profiling statistics."""
        stats = {}
        for name, times in self.timings.items():
            stats[name] = {
                "count": len(times),
                "total": sum(times),
                "mean": np.mean(times),
                "std": np.std(times),
                "min": min(times),
                "max": max(times),
            }
        return stats

    def print_stats(self):
        """Print profiling statistics."""
        if not self.enabled:
            print("Profiling is disabled")
            return

        print("\nPerformance Profile:")
        print("-" * 60)
        print(f"{'Operation':<30} {'Count':<8} {'Mean':<10} {'Total':<10}")
        print("-" * 60)

        stats = self.get_stats()
        for name, stat in stats.items():
            print(
                f"{name:<30} {stat['count']:<8} {stat['mean']:<10.3f} {stat['total']:<10.3f}"
            )

        print("-" * 60)

    def reset(self):
        """Reset all profiling data."""
        self.timings = {}


class ProfileContext:
    """Context manager for profiling."""

    def __init__(self, profiler: PerformanceProfiler, name: str):
        self.profiler = profiler
        self.name = name
        self.start_time = None

    def __enter__(self):
        if self.profiler.enabled:
            self.start_time = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.profiler.enabled and self.start_time is not None:
            duration = time.time() - self.start_time
            self.profiler.record(self.name, duration)


def optimize_for_inference(model: nn.Module, use_fp16: bool = False) -> nn.Module:
    """
    Optimize model for faster inference.

    Args:
        model: Model to optimize
        use_fp16: Whether to use half-precision (FP16)

    Returns:
        Optimized model

    Example:
        >>> from autotimm.interpretation.optimization import optimize_for_inference
        >>>
        >>> model = ImageClassifier(backbone="resnet50", num_classes=10)
        >>> model = optimize_for_inference(model, use_fp16=True)
        >>> # 2-3x faster inference on GPU
    """
    model.eval()

    # Disable gradient computation
    for param in model.parameters():
        param.requires_grad = False

    # Convert to half precision if requested
    if use_fp16 and torch.cuda.is_available():
        model = model.half()
        warnings.warn(
            "Using FP16. Make sure to convert inputs to half precision as well.",
            UserWarning,
        )

    # Enable cudnn benchmarking for faster convolutions
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True

    return model


__all__ = [
    "ExplanationCache",
    "BatchProcessor",
    "PerformanceProfiler",
    "optimize_for_inference",
]
